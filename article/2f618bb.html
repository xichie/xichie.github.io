<!DOCTYPE html><html lang="zh-CN"><head><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description" content=""><title>梯度下降算法对比 | warm的博客</title><link rel="stylesheet" type="text/css" href="/css/style.css?v=0.0.0"><link rel="stylesheet" type="text/css" href="//cdn.jsdelivr.net/npm/normalize.css/normalize.min.css"><link rel="stylesheet" type="text/css" href="//cdn.jsdelivr.net/npm/purecss/build/pure-min.min.css"><link rel="stylesheet" type="text/css" href="//cdn.jsdelivr.net/npm/purecss/build/grids-responsive-min.css"><link rel="stylesheet" href="//cdn.jsdelivr.net/npm/font-awesome@4.7.0/css/font-awesome.min.css"><script type="text/javascript" src="//cdn.jsdelivr.net/npm/jquery/dist/jquery.min.js"></script><link rel="icon" mask="" sizes="any" href="/favicon.ico"><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/apple-touch-icon.png"><script type="text/javascript" src="//cdn.jsdelivr.net/npm/clipboard/dist/clipboard.min.js"></script><script type="text/javascript" src="//cdn.jsdelivr.net/gh/codeseven/toastr/build/toastr.min.js"></script><link rel="stylesheet" href="//cdn.jsdelivr.net/gh/codeseven/toastr/build/toastr.min.css"><meta name="generator" content="Hexo 6.0.0"></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">梯度下降算法对比</h1><a id="logo" href="/.">warm的博客</a><p class="description"></p></div><div id="nav-menu"><a class="current" href="/."><i class="fa fa-home"> 首页</i></a><a href="/archives/"><i class="fa fa-archive"> 归档</i></a><a href="/about/"><i class="fa fa-user"> 关于</i></a><a href="/guestbook/"><i class="fa fa-comment"> 留言</i></a><a href="/atom.xml"><i class="fa fa-rss"> 订阅</i></a></div></div><div class="pure-g" id="layout"><div class="pure-u-1 pure-u-md-3-4"><div class="content_container"><div class="post"><h1 class="post-title">梯度下降算法对比</h1><div class="post-meta">2020-05-18<span> | </span><span class="category"><a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a></span><script src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" async></script><span id="busuanzi_container_page_pv"> | <span id="busuanzi_value_page_pv"></span><span> 阅读</span></span><span class="post-time"><span class="post-meta-item-text"> | </span><span class="post-meta-item-icon"><i class="fa fa-keyboard-o"></i><span class="post-count"> 4k</span><span class="post-meta-item-text"> 字</span></span></span><span class="post-time"> | <span class="post-meta-item-icon"><i class="fa fa-clock-o"></i><span class="post-count"> 17</span><span class="post-meta-item-text"> 分钟</span></span></span></div><div class="clear"><div class="toc-article" id="toc"><div class="toc-title">文章目录</div><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%96%B9%E6%B3%95%E6%80%BB%E7%BB%93"><span class="toc-number">1.</span> <span class="toc-text">梯度下降方法总结</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E7%A4%BA%E4%BE%8B%EF%BC%88%E5%9B%9E%E5%BD%92%E9%97%AE%E9%A2%98%EF%BC%89"><span class="toc-number">1.1.</span> <span class="toc-text">梯度下降示例（回归问题）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%EF%BC%9A"><span class="toc-number">1.2.</span> <span class="toc-text">梯度下降：</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Gradient-Desent-with-leraning-rate-decay"><span class="toc-number">1.2.1.</span> <span class="toc-text">Gradient Desent with leraning rate decay</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%9B%B4%E6%96%B0%E5%85%AC%E5%BC%8F%EF%BC%9A"><span class="toc-number">1.2.1.1.</span> <span class="toc-text">更新公式：</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Adagrad"><span class="toc-number">1.2.2.</span> <span class="toc-text">Adagrad</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#RMSprop"><span class="toc-number">1.2.3.</span> <span class="toc-text">RMSprop</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Momentum"><span class="toc-number">1.3.</span> <span class="toc-text">Momentum</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Adam"><span class="toc-number">1.4.</span> <span class="toc-text">Adam</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%80%BB%E7%BB%93"><span class="toc-number">1.5.</span> <span class="toc-text">总结</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99"><span class="toc-number">1.6.</span> <span class="toc-text">参考资料</span></a></li></ol></li></ol></div></div><div class="post-content"><h1 id="梯度下降方法总结"><a href="#梯度下降方法总结" class="headerlink" title="梯度下降方法总结"></a><center>梯度下降方法总结</center></h1><p>完整的notebook上传到了github上：<a target="_blank" rel="noopener" href="https://github.com/xichie/DeepLearning">https://github.com/xichie/DeepLearning</a></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> mpl_toolkits.mplot3d <span class="keyword">import</span> Axes3D</span><br><span class="line"><span class="keyword">from</span> IPython.display <span class="keyword">import</span> Image</span><br></pre></td></tr></table></figure>

<p><strong>深度学习中，我们最常用的优化算法就是基于梯度的方式了。所以在这里总结一下我目前所能理解的方法。（PS：在写这个的过程中，我发现指数平均无处不在啊。这个在可视化模型的Loss值随迭代次数的改变，为了使图像看起来更加平滑，用的也是指数平均。看来是该了解一下了！）</strong></p>
<h2 id="梯度下降示例（回归问题）"><a href="#梯度下降示例（回归问题）" class="headerlink" title="梯度下降示例（回归问题）"></a>梯度下降示例（回归问题）</h2><p><strong>示例中的数据是自己生成的，真实数据符合：</strong> $$y &#x3D; 3x + 5$$ </p>
<p><strong>构建$y &#x3D; wx + b$模型拟合数据，  通过梯度下降的方法更新参数w,b。比较几种梯度下降更新方式的优缺点。<br>目标函数为均方误差：$$Loss &#x3D; \frac{1}{N}\sum_{i&#x3D;0}^{N}\frac{1}{2}(\hat{y}^{(i)} - y^{(i)})^2$$</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x = np.arange(<span class="number">0</span>, <span class="number">20</span>)</span><br><span class="line">y = <span class="number">3</span> * x + <span class="number">5</span> </span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">plt.scatter(x, y)</span><br><span class="line">plt.xlabel(<span class="string">&#x27;w&#x27;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;b&#x27;</span>)</span><br><span class="line">plt.title(<span class="string">&#x27;Data&#x27;</span>)</span><br><span class="line"></span><br><span class="line">fig = plt.figure()</span><br><span class="line">ax = Axes3D(fig)</span><br><span class="line">w = np.linspace(-<span class="number">5</span>, <span class="number">5</span>)</span><br><span class="line">b = np.linspace(-<span class="number">5</span>, <span class="number">5</span>)</span><br><span class="line">w, b = np.meshgrid(w, b)</span><br><span class="line">loss = []</span><br><span class="line"><span class="keyword">for</span> w_i, b_i <span class="keyword">in</span> <span class="built_in">zip</span>(w.flatten(), b.flatten()):</span><br><span class="line">    y_hat = w_i * x + b_i</span><br><span class="line">    loss.append(np.mean((y_hat - y) ** <span class="number">2</span>) / <span class="number">2</span>)</span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line">ax.scatter(<span class="number">3</span>, <span class="number">5</span>, <span class="number">0</span>, c=<span class="string">&#x27;r&#x27;</span>,s=<span class="number">30</span>**<span class="number">2</span>, marker=<span class="string">&#x27;+&#x27;</span>)                     <span class="comment">#最优解</span></span><br><span class="line">ax.plot_surface(w, b, np.array(loss).reshape(<span class="number">50</span>, <span class="number">50</span>), cmap=<span class="string">&#x27;rainbow&#x27;</span>,alpha=<span class="number">1</span>)</span><br><span class="line">ax.set_xlabel(<span class="string">&#x27;w&#x27;</span>)</span><br><span class="line">ax.set_ylabel(<span class="string">&#x27;b&#x27;</span>)</span><br><span class="line">ax.set_zlabel(<span class="string">&#x27;Loss&#x27;</span>)</span><br></pre></td></tr></table></figure>




<pre><code>Text(0.5,0,&#39;Loss&#39;)
</code></pre>
<p><img src="https://img-blog.csdnimg.cn/20181217144638848.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzI2OTcyNzM1,size_16,color_FFFFFF,t_70"><br><img src="https://img-blog.csdnimg.cn/20181217144649206.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzI2OTcyNzM1,size_16,color_FFFFFF,t_70"></p>
<h2 id="梯度下降："><a href="#梯度下降：" class="headerlink" title="梯度下降："></a>梯度下降：</h2><p> <strong>深度学习中给一个目标函数$L(w)$  ，我们的目标是找到令其最小化的一组参数$w$。目前最常用的就是梯度下降了，找到一个方向，令当前的$w$向该方向移动从而减小目标函数的值。这个方向就是梯度的负方向（具体证明）。所以$w$的更新公式为：</strong>$$\large w^{t+1} &#x3D; w^{t} - \eta g^t$$<br><strong>其中$\eta$为步长或者学习率，$g^t &#x3D; \frac{\partial L}{\partial w}$即第t次迭代，$L$对$w$的梯度。<br>但上面这种更新参数的方式存在一个问题，就是$\eta$如何取值，如果取值过大就很有可能跳过最优点，取值过小那么更新的就会很慢。我们在更新参数的过程中，一开始离最优解比较远，我们希望步伐迈的可以大一点，当离最优解越来越近的时候，我们希望步伐要小一点，因此有人就提出一种随着迭代次数的增加步长逐渐减小的更新方式：$$\large w^{t+1} &#x3D; w^t - \eta^t g^t$$<br>其中$\eta^t &#x3D; \frac{\eta}{\sqrt{ t+1 }}$,$\eta$ 为 步长或者学习率。<br></strong><br><strong>下面是计算参数w，b梯度以及可视化更新过程代码。</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 计算w, b的梯度</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">compute_grad</span>(<span class="params">paramters, w, b</span>):</span><br><span class="line">    y_hat = paramters[<span class="string">&#x27;y_hat&#x27;</span>]</span><br><span class="line">    y =  paramters[<span class="string">&#x27;y&#x27;</span>]</span><br><span class="line">    x = paramters[<span class="string">&#x27;x&#x27;</span>]</span><br><span class="line">   </span><br><span class="line">    grad_w = np.<span class="built_in">sum</span>((y_hat - y) * x)</span><br><span class="line">    grad_b = np.<span class="built_in">sum</span>((y_hat - y))</span><br><span class="line">    <span class="keyword">return</span> grad_w, grad_b</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 可视化每次参数的更新</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">plot_history</span>(<span class="params">w_history, b_history, title=<span class="literal">None</span></span>):</span><br><span class="line">    plt.scatter(<span class="number">3.</span>, <span class="number">5.</span>, c = <span class="string">&#x27;r&#x27;</span> ,s=<span class="number">50</span>**<span class="number">2</span>, marker=<span class="string">&#x27;+&#x27;</span>)</span><br><span class="line">    plt.scatter(w_history, b_history, s=<span class="number">20</span>, marker=<span class="string">&#x27;o&#x27;</span>)</span><br><span class="line">    plt.plot(w_history, b_history)</span><br><span class="line">    plt.xlabel(<span class="string">&#x27;w&#x27;</span>)</span><br><span class="line">    plt.ylabel(<span class="string">&#x27;b&#x27;</span>)</span><br><span class="line">    plt.title(title)</span><br><span class="line">    fig = plt.figure()</span><br><span class="line">    ax = Axes3D(fig)</span><br><span class="line">    w = np.linspace(-<span class="number">5</span>, <span class="number">5</span>)</span><br><span class="line">    b = np.linspace(-<span class="number">5</span>, <span class="number">5</span>)</span><br><span class="line">    w, b = np.meshgrid(w, b)</span><br><span class="line">    loss = []</span><br><span class="line">    <span class="keyword">for</span> w_i, b_i <span class="keyword">in</span> <span class="built_in">zip</span>(w.flatten(), b.flatten()):</span><br><span class="line">        y_hat = w_i * x + b_i</span><br><span class="line">        loss.append(np.mean((y_hat - y) ** <span class="number">2</span>) / <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">    w_history, b_history = np.array(w_history),np.array(b_history)</span><br><span class="line">    history = []</span><br><span class="line">    <span class="keyword">for</span> w_i, b_i <span class="keyword">in</span> <span class="built_in">zip</span>(w_history, b_history):</span><br><span class="line">        y_hat = w_i * x + b_i</span><br><span class="line">        history.append(np.mean((y_hat - y) ** <span class="number">2</span>) / <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">    ax.scatter(<span class="number">3</span>, <span class="number">5</span>, <span class="number">0</span>, <span class="string">&#x27;k+&#x27;</span>,s=<span class="number">20</span>**<span class="number">2</span>)                     <span class="comment">#最优解</span></span><br><span class="line">    ax.plot(w_history, b_history, history, c=<span class="string">&#x27;k&#x27;</span>,alpha=<span class="number">1</span>)</span><br><span class="line">    ax.scatter(w_history, b_history, history, c=<span class="string">&#x27;k&#x27;</span>,s=<span class="number">10</span>**<span class="number">2</span>, marker=<span class="string">&#x27;o&#x27;</span>)</span><br><span class="line">    ax.plot_surface(w, b, np.array(loss).reshape(<span class="number">50</span>, <span class="number">50</span>), cmap=<span class="string">&#x27;rainbow&#x27;</span>,alpha=<span class="number">1</span>)</span><br><span class="line">    ax.set_xlabel(<span class="string">&#x27;w&#x27;</span>)</span><br><span class="line">    ax.set_ylabel(<span class="string">&#x27;b&#x27;</span>)</span><br><span class="line">    ax.set_zlabel(<span class="string">&#x27;Loss&#x27;</span>)</span><br></pre></td></tr></table></figure>

<h3 id="Gradient-Desent-with-leraning-rate-decay"><a href="#Gradient-Desent-with-leraning-rate-decay" class="headerlink" title="Gradient Desent with leraning rate decay"></a>Gradient Desent with leraning rate decay</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">iteration = <span class="number">100000</span></span><br><span class="line">w = b = <span class="number">0</span></span><br><span class="line">lr = <span class="number">0.001</span></span><br><span class="line">parameters = &#123;&#125;</span><br><span class="line">parameters[<span class="string">&#x27;y&#x27;</span>] = y</span><br><span class="line">parameters[<span class="string">&#x27;x&#x27;</span>] = x</span><br><span class="line"></span><br><span class="line">w_history = [w]</span><br><span class="line">b_history = [b]</span><br><span class="line">loss_history = []</span><br><span class="line"><span class="keyword">for</span> t <span class="keyword">in</span> <span class="built_in">range</span>(iteration):</span><br><span class="line">    y_hat = w * x + b</span><br><span class="line">    parameters[<span class="string">&#x27;y_hat&#x27;</span>] = y_hat</span><br><span class="line">    loss_t = np.mean((y_hat - y) ** <span class="number">2</span>) / <span class="number">2</span>             <span class="comment"># mse loss</span></span><br><span class="line">    grad_w, grad_b = compute_grad(parameters, w, b)</span><br><span class="line">    w = w - lr / np.sqrt(t + <span class="number">1</span>) * grad_w</span><br><span class="line">    b = b - lr / np.sqrt(t + <span class="number">1</span>) * grad_b</span><br><span class="line">    w_history.append(w)</span><br><span class="line">    b_history.append(b)</span><br><span class="line">    loss_history.append(loss_t)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;After iteration 1000000 w = &quot;</span>, w)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;After iteration 1000000 b = &quot;</span>, b)</span><br></pre></td></tr></table></figure>

<pre><code>After iteration 1000000 w =  3.0124698201218907
After iteration 1000000 b =  4.838243650840835
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">plot_history(w_history, b_history, title=<span class="string">&#x27;Gradient desent with learning rate decay&#x27;</span>)</span><br></pre></td></tr></table></figure>
<p><img src="https://img-blog.csdnimg.cn/20181217144832880.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzI2OTcyNzM1,size_16,color_FFFFFF,t_70"><br><img src="https://img-blog.csdnimg.cn/20181217144841166.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzI2OTcyNzM1,size_16,color_FFFFFF,t_70"></p>
<p><strong><font color=#FF0000 size=3>上图可以看出普通的梯度更新方式，在迭代了10万次后才收敛到最优值(红色十字)，并且有震荡的现象。</font></strong>  </p>
<p><strong>上述参数更新方法，学习率随着迭代次数的增加就会越来越小。但是这么做还是有些不足，因为所有参数的学习率都是一样的，这是很不科学的，因为对于L的不同维度而言。它距离最优点的距离是不一样的，如下图：</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Image(filename=<span class="string">&#x27;./1.png&#x27;</span>)</span><br></pre></td></tr></table></figure>
<p><img src="https://img-blog.csdnimg.cn/20181217144932229.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzI2OTcyNzM1,size_16,color_FFFFFF,t_70"><br><strong>图中在 （水平）方向上loss的变化明显要比 （垂直）方向上的变化要明显 。我们如果在更新$w_1$和$w_2$时使用相同的学习率显然是不合适的。<br>我们从上面的的结果中，也可以很清楚的看出传统梯度下降更新参数的缺点，参数w很快就收敛到了最优，但是参数b离最优值还很远。造成这种现象就是因为两个参数每次迭代步长都一样。对于不同的参数我们希望能够有属于自己的学习率，因此提出了一种叫做Adagrad的更新方式。</strong></p>
<h4 id="更新公式："><a href="#更新公式：" class="headerlink" title="更新公式："></a>更新公式：</h4><p>$$\large w^{t+1} &#x3D; w^t - \frac{\eta}{\sqrt{\sum_{i&#x3D;0}^{t}(g^i)^2}}g^t$$<br><strong>对于参数$w$每次更新的学习率要除以原来所有$w$梯度的平方根。这样对于每一个参数更新的速率都不一样，但是这样做的会产生什么影响呢？直观上的理解，随着算法不断迭代，分母会越来越大，整体的学习率会越来越小。所以，一般来说Adagrad算法一开始是激励收敛，到了后面就慢慢变成惩罚收敛，速度越来越慢。我们的希望也就达到了。</strong></p>
<p><strong>这么做有什么依据吗？它当然不是凭空得到的。给一个目标函数$L(w)$，求其最小值点，这不就是一个函数求极值的问题吗？我们知道当一阶导数等于0二阶导数大于0时函数有极小值。  给定参数  我们用二阶Taylor公式在这一点展开，得到函数：$$f(w)&#x3D;L(w^t)+L^{‘}(w^t)(w - w^t) + \frac{1}{2}L^{‘’}(w^t)(w-w^t)^2$$</strong><br><strong>$f$就是$L$在$w^t$处的二阶Taylor近似，显然$f$在$w^t$处的值与$L$相等。  我们想要更新参数$w$，根据上面的介绍我们可以找其梯度的反方向乘以一个学习率更新。现在我们可以将这个将这个问题变为寻找$f$的极值，因为$f$是一个二次函数(凸函数)，所以我们令$f$对$w$求导等于0就可以得到极值点:$$w^{t+1} &#x3D; w^t - \frac{L^{‘}(w^t)}{L^{‘’}(w^t)}$$ 这就是在$w^t$处，更新w最优的步长。这种方法就叫做牛顿法。最理想的情况下，牛顿法只需要一次迭代就可以找到最优解。我们跟上述的梯度下降更新公式比较，牛顿法在更新参数$w$的过程中，迈步的大小是跟二阶导数成反比的，也就是说学习率$\eta$实际上替代的是二阶导数。而在Adagrad中使用累计梯度的均方根（root mean square）来近似二阶导数。总之，步长是跟一阶导和二阶导都有关系的，但是二阶导计算复杂度比较高，所以我们设法用其他的方式来近似它或者简化它。</strong></p>
<h3 id="Adagrad"><a href="#Adagrad" class="headerlink" title="Adagrad"></a>Adagrad</h3><p><strong>在参数更新的方式换成Adagrad后，只迭代了1500次，参数w和b就达到了最优。</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">iteration = <span class="number">1500</span></span><br><span class="line">w = b = <span class="number">0</span></span><br><span class="line">lr = <span class="number">1</span></span><br><span class="line">lr_w = <span class="number">0</span></span><br><span class="line">lr_b = <span class="number">0</span></span><br><span class="line">parameters = &#123;&#125;</span><br><span class="line">parameters[<span class="string">&#x27;y&#x27;</span>] = y</span><br><span class="line">parameters[<span class="string">&#x27;x&#x27;</span>] = x</span><br><span class="line">w_history = [w]</span><br><span class="line">b_history = [b]</span><br><span class="line">loss_history = []</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(iteration):</span><br><span class="line">    y_hat = w * x + b</span><br><span class="line">    parameters[<span class="string">&#x27;y_hat&#x27;</span>] = y_hat</span><br><span class="line">    loss_i = np.mean((y_hat - y) ** <span class="number">2</span>) / <span class="number">2</span>             <span class="comment"># mse loss</span></span><br><span class="line">    grad_w, grad_b = compute_grad(parameters, w, b)</span><br><span class="line">    lr_w += grad_w ** <span class="number">2</span>                                <span class="comment"># sum w_t ** 2</span></span><br><span class="line">    lr_b += grad_b ** <span class="number">2</span>                                <span class="comment"># sum b_t ** 2</span></span><br><span class="line">    w = w - lr / np.sqrt(lr_w) * grad_w</span><br><span class="line">    b = b - lr / np.sqrt(lr_b) * grad_b</span><br><span class="line">    w_history.append(w)</span><br><span class="line">    b_history.append(b)</span><br><span class="line">    loss_history.append(loss_i)</span><br><span class="line"><span class="built_in">print</span>(w)</span><br><span class="line"><span class="built_in">print</span>(b)</span><br></pre></td></tr></table></figure>

<pre><code>3.0000691771084793
4.999123082502581
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">plot_history(w_history, b_history, title=<span class="string">&#x27;Adagrad&#x27;</span>)</span><br></pre></td></tr></table></figure>

<p><img src="https://img-blog.csdnimg.cn/20181217145041650.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzI2OTcyNzM1,size_16,color_FFFFFF,t_70"><br><img src="https://img-blog.csdnimg.cn/20181217145101164.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzI2OTcyNzM1,size_16,color_FFFFFF,t_70"></p>
<p><strong><font color=#FF0000 size=3>上图可以看出Adagrad方法在迭代了1500次后就已经达到最优值(红色十字)，收敛速度有明显的提升，并且也比较稳定。</font></strong>  </p>
<p><strong>图中我们可以还可以看出Adagrad学习率衰减的过快，为了解决这个问题，有一种叫做RMSprop或均方根反向传播算法，它是由传奇人物Geoffrey Hinton提出的，当时只是在课堂上是随意提出的一个想法。（神一般的人物，都懒得发论文了。。。）更新公式：</strong>   </p>
<p>$$\large w^t+1 &#x3D; w^t - \frac{\eta}{\sigma^t}g^t$$  </p>
<p><strong>其中$\sigma^t &#x3D; \sqrt{\alpha(\sigma^{t-1})^2+(1-\alpha)(g^t)^2}$, $\eta$为学习率。</strong>    </p>
<p><strong>$\sigma^t$的计算公式，实际上是计算了梯度的指数平均值。它使得间隔和权重成比例变化，在计算步长时，用学习率除以$\sigma^t$，从而达到在其可以更快更平滑的向最优参数的方向移动。</strong></p>
<h3 id="RMSprop"><a href="#RMSprop" class="headerlink" title="RMSprop"></a>RMSprop</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">iteration = <span class="number">200</span></span><br><span class="line">w = b = <span class="number">0</span></span><br><span class="line">lr = <span class="number">0.1</span></span><br><span class="line">lr_w = <span class="number">0</span></span><br><span class="line">lr_b = <span class="number">0</span></span><br><span class="line">alpha = <span class="number">0.9</span></span><br><span class="line">parameters = &#123;&#125;</span><br><span class="line">parameters[<span class="string">&#x27;y&#x27;</span>] = y</span><br><span class="line">parameters[<span class="string">&#x27;x&#x27;</span>] = x</span><br><span class="line">w_history = [w]</span><br><span class="line">b_history = [b]</span><br><span class="line">loss_history = []</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(iteration):</span><br><span class="line">    y_hat = w * x + b</span><br><span class="line">    parameters[<span class="string">&#x27;y_hat&#x27;</span>] = y_hat</span><br><span class="line">    loss_i = np.mean((y_hat - y) ** <span class="number">2</span>) / <span class="number">2</span>             <span class="comment"># mse loss</span></span><br><span class="line">    grad_w, grad_b = compute_grad(parameters, w, b)</span><br><span class="line">    lr_w = alpha * lr_w + (<span class="number">1</span>-alpha) * (grad_w**<span class="number">2</span>)                                </span><br><span class="line">    lr_b = alpha * lr_b+ (<span class="number">1</span>-alpha) * (grad_b**<span class="number">2</span>)                               </span><br><span class="line">    w = w - lr / np.sqrt(lr_w) * grad_w</span><br><span class="line">    b = b - lr / np.sqrt(lr_b) * grad_b</span><br><span class="line">    w_history.append(w)</span><br><span class="line">    b_history.append(b)</span><br><span class="line">    loss_history.append(loss_i)</span><br><span class="line"><span class="built_in">print</span>(w)</span><br><span class="line"><span class="built_in">print</span>(b)</span><br></pre></td></tr></table></figure>

<pre><code>3.0500996419976674
5.0484005151171925
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">plot_history(w_history, b_history, title=<span class="string">&#x27;RMSprop&#x27;</span>)</span><br></pre></td></tr></table></figure>
<p><img src="https://img-blog.csdnimg.cn/20181217145204950.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzI2OTcyNzM1,size_16,color_FFFFFF,t_70"><br><img src="https://img-blog.csdnimg.cn/20181217145219570.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzI2OTcyNzM1,size_16,color_FFFFFF,t_70"></p>
<p><strong><font color=#FF0000 size=3>上图可以看出，RMSprop只用了不到200次迭代就达到了最优值，并且学习率衰减的也很平缓，它同样也很稳定。</font></strong></p>
<p><strong>RMSprop更新参数的方式跟Adagrad相比，主要是解决了Adagrad学习率衰减过快的问题，这个问题在模型比较复杂时非常重要，可以在一定程度上防止在参数更新时跳过最优值。</strong></p>
<p><strong>上面提到的对原始梯度方法更新参数的改进，关注点都为学习率衰减这个问题。但在模型比较复杂的情况下，还存在一个非常非常重要的问题，那就是局部最优。 如何避免参数陷入局部最优，下面介绍的这种Momentum方法，在一定程度上缓解了这个问题，并且还可以加快收敛速度。</strong><br><strong>Momentum是一种动量的梯度下降方式，那么什么是动量呢？我认为就是每次在参数更新时，都会有一个初始的速度。原始的梯度下降每次迭代更新参数的过程中，我们都会在计算当前位置梯度，然后去更新参数，我们上面已经介绍过了，这种更新方式会在一些对梯度敏感的参数方向上来回震荡。Momentum解决这个问题的方式就是，在更新当前参数时考虑原来的上一次的梯度。通俗的理解就是，一个人如果在原地静止不动，让他突然来一个大转弯，那么他很轻松的就可以转过来。如果这个人正在向一个方向奔跑，让他突然来一个大转弯，那显然很吃力，一般的情况下我们会有一个弧度慢慢的转过来。Momentum就是这样更新参数的，在每次更新参数时，用上一次的梯度乘以一个系数$\beta$加上这一次的梯度，用这个累加的梯度去更新。在当前梯度很小或者为0时，由于上一次梯度的存在，参数也会更新，这样在一定程度上也防止了陷入局部最优的情况。更新公式:</strong><br>$$v^0 &#x3D; 0$$<br>$$g^t &#x3D; \frac{\partial{L}}{\partial{w^t}}$$<br>$$v^{t+1} &#x3D; \beta v^t + (1 - \beta)g^t$$<br>$$w^{t+1} &#x3D; w^{t} - \eta v^{t+1}$$<br><strong>其中$\beta$为一个[0, 1)之间的常数，当$\beta$趋近于1，表示我们更看重上一次的梯度，反之，表示更看重当前的梯度。$\eta$为学习率。</strong></p>
<h2 id="Momentum"><a href="#Momentum" class="headerlink" title="Momentum"></a>Momentum</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">iteration = <span class="number">1000</span></span><br><span class="line">w = b = <span class="number">0</span></span><br><span class="line">lr = <span class="number">0.001</span></span><br><span class="line">v_w = <span class="number">0</span></span><br><span class="line">v_b = <span class="number">0</span></span><br><span class="line">beta = <span class="number">0.9</span></span><br><span class="line">parameters = &#123;&#125;</span><br><span class="line">parameters[<span class="string">&#x27;y&#x27;</span>] = y</span><br><span class="line">parameters[<span class="string">&#x27;x&#x27;</span>] = x</span><br><span class="line">w_history = [w]</span><br><span class="line">b_history = [b]</span><br><span class="line">loss_history = []</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(iteration):</span><br><span class="line">    y_hat = w * x + b</span><br><span class="line">    parameters[<span class="string">&#x27;y_hat&#x27;</span>] = y_hat</span><br><span class="line">    loss_i = np.mean((y_hat - y) ** <span class="number">2</span>) / <span class="number">2</span>             <span class="comment"># mse loss</span></span><br><span class="line">    grad_w, grad_b = compute_grad(parameters, w, b)</span><br><span class="line">    v_w = beta * v_w + (<span class="number">1</span> - beta) * grad_w  </span><br><span class="line">    v_b = beta * v_b + (<span class="number">1</span> - beta) * grad_b</span><br><span class="line">    w = w - lr * v_w</span><br><span class="line">    b = b - lr * v_b</span><br><span class="line">    w_history.append(w)</span><br><span class="line">    b_history.append(b)</span><br><span class="line">    loss_history.append(loss_i)</span><br><span class="line"><span class="built_in">print</span>(w)</span><br><span class="line"><span class="built_in">print</span>(b)</span><br></pre></td></tr></table></figure>

<pre><code>3.0013497438420527
4.982491356406397
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">plot_history(w_history, b_history, title=<span class="string">&#x27;Momentum&#x27;</span>)</span><br></pre></td></tr></table></figure>

<p><img src="https://img-blog.csdnimg.cn/20181217145534105.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzI2OTcyNzM1,size_16,color_FFFFFF,t_70"></p>
<p><img src="https://img-blog.csdnimg.cn/20181217145546955.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzI2OTcyNzM1,size_16,color_FFFFFF,t_70"></p>
<p><strong><font color=#FF0000 size=3>上图可以看出，Momentum在一开始在参数w方向震荡的还是比较严重的，这是因为一开始梯度的累加还很小，但是很快随着梯度的累加，它所占的比重越来越大，震荡的现象开始逐渐的缓和，并向着最优的方向直线前进。</font></strong></p>
<p><strong>上面介绍的这几种梯度更新方法，原始学习率衰减的梯度下降、Adagrad、RMSprop这几个考虑的学习率衰减的改进，Momentum考虑的是如何能够找到一个更好且更平滑的更新方向，且可以快速的到达最优解。下面介绍的梯度下降方法叫做Adam，它可以看做是RMSprop和Momentum的结合，它同时拥有两者的优点，所以它也是目前在深度学习中最最最流行的更新方式。具体的也不说了，很好用就得了。公式太多直接引用<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1412.6980">论文</a>中的：</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Image(filename=<span class="string">&#x27;./adam.png&#x27;</span>)</span><br></pre></td></tr></table></figure>
<p><img src="https://img-blog.csdnimg.cn/20181217145608932.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzI2OTcyNzM1,size_16,color_FFFFFF,t_70"></p>
<h2 id="Adam"><a href="#Adam" class="headerlink" title="Adam"></a>Adam</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line">iteration = <span class="number">1000</span></span><br><span class="line">w = b = <span class="number">0</span></span><br><span class="line">epslion=<span class="number">1e-8</span></span><br><span class="line">lr = <span class="number">0.01</span></span><br><span class="line">v_w = <span class="number">0</span></span><br><span class="line">v_b = <span class="number">0</span></span><br><span class="line">m_w = <span class="number">0</span></span><br><span class="line">m_b =<span class="number">0</span></span><br><span class="line">beta1 = <span class="number">0.1</span></span><br><span class="line">beta2 = <span class="number">0.6</span></span><br><span class="line">parameters = &#123;&#125;</span><br><span class="line">parameters[<span class="string">&#x27;y&#x27;</span>] = y</span><br><span class="line">parameters[<span class="string">&#x27;x&#x27;</span>] = x</span><br><span class="line"><span class="comment"># for beta1 in np.arange(0, 1, step=0.1):</span></span><br><span class="line"><span class="comment">#     for beta2 in np.arange(0, 1, step=0.1):</span></span><br><span class="line">w = b = <span class="number">0</span></span><br><span class="line">w_history = [w]</span><br><span class="line">b_history = [b]</span><br><span class="line">loss_history = []</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(iteration):</span><br><span class="line">    y_hat = w * x + b</span><br><span class="line">    parameters[<span class="string">&#x27;y_hat&#x27;</span>] = y_hat</span><br><span class="line">    loss_i = np.mean((y_hat - y) ** <span class="number">2</span>) / <span class="number">2</span>             <span class="comment"># mse loss</span></span><br><span class="line">    grad_w, grad_b = compute_grad(parameters, w, b)</span><br><span class="line">    m_w = beta1 * m_w + (<span class="number">1</span> - beta1) * grad_w</span><br><span class="line">    m_b = beta1 * m_w + (<span class="number">1</span> - beta1) * grad_b</span><br><span class="line">    v_w = beta2 * v_w + (<span class="number">1</span> - beta2) * (grad_w ** <span class="number">2</span>)</span><br><span class="line">    v_b = beta2 * v_b + (<span class="number">1</span> - beta2) * (grad_b ** <span class="number">2</span>)</span><br><span class="line">    m_w_hat = m_w / (<span class="number">1</span> - beta1 ** (i+<span class="number">1</span>))</span><br><span class="line">    m_b_hat = m_b / (<span class="number">1</span> - beta1 ** (i+<span class="number">1</span>))</span><br><span class="line">    v_w_hat = v_w / (<span class="number">1</span> - beta2 ** (i+<span class="number">1</span>))</span><br><span class="line">    v_b_hat = v_b / (<span class="number">1</span> - beta2 ** (i+<span class="number">1</span>))</span><br><span class="line">    w = w - lr * m_w_hat / (np.sqrt(v_w_hat) + epslion)</span><br><span class="line">    b = b - lr * m_b_hat / (np.sqrt(v_b_hat) + epslion)</span><br><span class="line">    w_history.append(w)</span><br><span class="line">    b_history.append(b)</span><br><span class="line">    loss_history.append(loss_i)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;beta1 = %f, beta2 = %f&#x27;</span>%(beta1, beta2))</span><br><span class="line"><span class="built_in">print</span>(w)</span><br><span class="line"><span class="built_in">print</span>(b)</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<pre><code>beta1 = 0.100000, beta2 = 0.600000
3.0040907948652804
5.009537540692813
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">plot_history(w_history, b_history, title=<span class="string">&#x27;Adam&#x27;</span>)</span><br></pre></td></tr></table></figure>

<p><img src="https://img-blog.csdnimg.cn/20181217145639103.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzI2OTcyNzM1,size_16,color_FFFFFF,t_70"><br><img src="https://img-blog.csdnimg.cn/20181217145648458.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzI2OTcyNzM1,size_16,color_FFFFFF,t_70"></p>
<p><strong>从上面的结果中，貌似Adam的表现还不如RMSprop，可能是我写的例子太简单了。。。。。根据别人的经验，在模型复杂时，特别是深度神经网络中，不妨优先试试Adam，它的效果一般都不错。<font color=#FF0000>（PS：Ng说的）</font></strong></p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p><strong>由于现在各种深度学习toolkits的出现,比如Keras(我的最爱)，Tensorflow，Pytorch（12月刚发布了1.0版本，貌似很厉害。。。。）等，人们可以不用太去关系梯度的计算和反向传播这些问题，极大的简化了从理论到实践的过程。我们只要有一个好的想法，可以很方便快速的利用这些工具去实现它。我认为这是目前深度学习这么火很重要的原因，试想一下，如果都从底层造轮子(我并不反对这个)，那这个难度就可以把绝大部分人挡在深度学习的大门外，说了一堆废话。。。。。回归正题。<br>总结这个最大的收获，自己要理解这些方法的基本原理是什么，当以后使用它的时候，最起码知道，每种方法的优势在哪儿，每个参数的意义是什么。在使用这些toolkits时,才能做到心中有数。</strong> </p>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p>[1] 李宏毅，台湾大学. <a target="_blank" rel="noopener" href="http://speech.ee.ntu.edu.tw/~tlkagk/courses.html">Machine Learning (2017,Fall) </a>.<br>[2] 深度学习（中文版）（Ian Goodfellow, Yoshua Bengio, Aaron Courville)<br>[3] Kingma D P , Ba J . <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1412.6980">Adam: A Method for Stochastic Optimization[J]</a>. Computer Science, 2014.</p>
</div><div id="donate"><link rel="stylesheet" type="text/css" href="/css/donate.css"><script type="text/javascript" src="/js/donate.js" successtext="复制成功!"></script><a class="pos-f tr3" id="github" target="_blank" rel="noopener" href="https://github.com/Kaiyuan/donate-page" arget="_blank" title="Github"></a><div id="DonateText">Donate</div><ul class="list pos-f" id="donateBox"><li id="AliPay" qr="/sponsor-page/simple/images/AliPayQR.jpg"></li><li id="WeChat" qr="/sponsor-page/simple/images/WeChatQR.JPG"></li></ul><div class="pos-f left-100" id="QRBox"><div id="MainBox"></div></div></div><div class="post-copyright"><script type="text/javascript" src="/js/copyright.js" successtext="复制成功!"></script><link rel="stylesheet" type="text/css" href="/css/copyright.css"><p><span>本文标题：</span>梯度下降算法对比</p><p><span>文章作者：</span>warm</p><p><span>发布时间：</span>2020-05-18</p><p><span>最后更新：</span>2022-02-23</p><p><span>原始链接：</span><a href="/article/2f618bb.html">http://xichie.github.io/article/2f618bb.html</a><span class="copy-path"><i class="fa fa-clipboard" data-clipboard-text="http://xichie.github.io/article/2f618bb.html"></i></span></p><p><span>版权声明：</span>The author owns the copyright, please indicate the source reproduced.</p></div><br><script type="text/javascript" src="/js/share.js?v=0.0.0" async></script><a class="article-share-link" data-url="http://xichie.github.io/article/2f618bb.html" data-id="ckzz980j20017sku9f8or28hl" data-qrcode="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAMYAAADGCAAAAACs8KCBAAACLklEQVR42u3awW6EMAxFUf7/p6nU3aidzH02jIRzs6qmQHJYGMfOceBx/g5+zflnkN/5XMUhQ4aMxzLO5SDTrO8li+Psd8+XIUPGDgwSQHl4TZ9JXt+HNcuQIUMGCMdk8KfJkCFDxlUBl4RXEmT5vDJkyNiZkYZInuqtp68lnTJkyNiNkTYGvvn3jf0NGTJkPIRxNkbtyVfN/vJMGTJkjGas07ja0YrOUQy+AT7IDTJkyBjB6BTU+C+8ZMbf/T/fDRkyZIxm9NuHacrIAzf6DMiQIWM0o7a4ddugVu5PV4LSRBkyZIxg8OSMTMb/m16PerAyZMgYyqhVq4LtZVieS1/i27ArQ4aMQQxeDqsd3qo1LNPTFDJkyJjN6LQb080tYfPyX1BNlCFDxsMZvPTfX2ga0HnYlSFDxj6Mq5K2Tmuz1lKVIUPGDozOwa+0rJ/ig82qDBkyhjLSkHftXbxJwNsPMmTImMfgAZeH2s5Wdh2gP/RgZciQMZSRFtf4wYh+Ehl/N2TIkLEBg6drpBmQ3pUG3HivLEOGjBGMWsDtY0jgRi0HGTJkjGbUQnCnwVkr+rf6HjJkyHgs4wwHT+bSAlx6zcsvMmTIGM3gg4fUzqENUsiTIUPGnoyrtpf3bXfRu5chQ8YGDB74eMy+6hpUYpMhQ4aM0qGutGCXNgZkyJAhozZljd25UoYMGfswagslbchay+HGvbgMGTIeyOg3BmqpXtr47BzUkCFDxmMZPxtJslAOTX2RAAAAAElFTkSuQmCC">分享</a><div class="tags"><a href="/tags/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D/"><i class="fa fa-tag"></i>梯度下降</a></div><div class="post-nav"><a class="pre" href="/article/5f366732.html">JSP学习笔记</a><a class="next" href="/article/a5b5607a.html">web相关知识</a></div><div id="lv-container" data-id="city" data-uid="MTAyMC80MzQ2MS8yMDAwMQ=="><script>(function(d, s) {
   var j, e = d.getElementsByTagName(s)[0];
   if (typeof LivereTower === 'function') { return; }
   j = d.createElement(s);
   j.src = 'https://cdn-city.livere.com/js/embed.dist.js';
   j.async = true;
   e.parentNode.insertBefore(j, e);
})(document, 'script');
</script></div></div></div></div><div class="pure-u-1-4 hidden_mid_and_down"><div id="sidebar"><div class="widget"><form class="search-form" action="//www.baidu.com/baidu" method="get" accept-charset="utf-8" target="_blank"><input type="search" name="word" maxlength="20" placeholder="Search"/><input type="hidden" name="si" value="http://xichie.github.io"/><input name="tn" type="hidden" value="bds"/><input name="cl" type="hidden" value="3"/><input name="ct" type="hidden" value="2097152"/><input name="s" type="hidden" value="on"/></form></div><div class="widget"><div class="widget-title"><i class="fa fa-folder-o"> 分类</i></div><ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/python/">python</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/web/">web</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/">大数据</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%97%A5%E5%B8%B8%E7%AC%94%E8%AE%B0/">日常笔记</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E7%AE%97%E6%B3%95/">算法</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80/">计算机基础</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-star-o"> 标签</i></div><div class="tagcloud"><a href="/tags/c%E8%AF%AD%E8%A8%80/" style="font-size: 15px;">c语言</a> <a href="/tags/%E7%AE%97%E6%B3%95/" style="font-size: 15px;">算法</a> <a href="/tags/PAT/" style="font-size: 15px;">PAT</a> <a href="/tags/python/" style="font-size: 15px;">python</a> <a href="/tags/web%E5%9F%BA%E7%A1%80/" style="font-size: 15px;">web基础</a> <a href="/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/" style="font-size: 15px;">大数据</a> <a href="/tags/%E5%B7%A5%E5%85%B7/" style="font-size: 15px;">工具</a> <a href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" style="font-size: 15px;">机器学习</a> <a href="/tags/GAN/" style="font-size: 15px;">GAN</a> <a href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" style="font-size: 15px;">深度学习</a> <a href="/tags/VAE/" style="font-size: 15px;">VAE</a> <a href="/tags/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D/" style="font-size: 15px;">梯度下降</a> <a href="/tags/%E6%A2%AF%E5%BA%A6/" style="font-size: 15px;">梯度</a> <a href="/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80/" style="font-size: 15px;">计算机基础</a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-file-o"> 最近文章</i></div><ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/article/32412401.html">论文笔记</a></li><li class="post-list-item"><a class="post-list-link" href="/article/9daba997.html">写作记录</a></li><li class="post-list-item"><a class="post-list-link" href="/article/8d566e8a.html">python3中的浅拷贝与深拷贝</a></li><li class="post-list-item"><a class="post-list-link" href="/article/bb9105ac.html">LeNet-5模型实现</a></li><li class="post-list-item"><a class="post-list-link" href="/article/8e63b6f8.html">MapReduce单元测试不通过</a></li><li class="post-list-item"><a class="post-list-link" href="/article/a27dd3ed.html">python读取cifar10数据集</a></li><li class="post-list-item"><a class="post-list-link" href="/article/775bc24d.html">KNN在MR和Spark下实现的IO操作比较</a></li><li class="post-list-item"><a class="post-list-link" href="/article/2bc44e46.html">神经网络参数初始化</a></li><li class="post-list-item"><a class="post-list-link" href="/article/9795e140.html">卷积网络反向传播过程</a></li><li class="post-list-item"><a class="post-list-link" href="/article/3254a3f1.html">VAE变分自编码器Keras实现</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-external-link"> 友情链接</i></div><ul></ul><a href="https://blog.csdn.net/qq_26972735" title="CSDN博客" target="_blank">CSDN博客</a></div></div></div><div class="pure-u-1 pure-u-md-3-4"><div id="footer">Copyright © 2022 <a href="/." rel="nofollow">warm的博客.</a> Powered by<a rel="nofollow" target="_blank" href="https://hexo.io"> Hexo.</a><a rel="nofollow" target="_blank" href="https://github.com/tufu9441/maupassant-hexo"> Theme</a> by<a rel="nofollow" target="_blank" href="https://github.com/pagecho"> Cho.</a></div></div></div><a class="show" id="rocket" href="#top"></a><script type="text/javascript" src="/js/totop.js?v=0.0.0" async></script><script type="text/javascript" src="//cdn.jsdelivr.net/gh/fancyapps/fancybox/dist/jquery.fancybox.min.js" async></script><script type="text/javascript" src="/js/fancybox.js?v=0.0.0" async></script><link rel="stylesheet" type="text/css" href="//cdn.jsdelivr.net/gh/fancyapps/fancybox/dist/jquery.fancybox.min.css"><script type="text/javascript" color="0,0,0" opacity="0.5" zIndex="-2" count="50" src="//cdn.jsdelivr.net/npm/canvas-nest.js/dist/canvas-nest.min.js"></script><script type="text/javascript" src="/js/love.js"></script><script type="text/javascript" src="/js/copycode.js" successtext="复制成功!"></script><link rel="stylesheet" type="text/css" href="/css/copycode.css"><script type="text/x-mathjax-config">MathJax.Hub.Config({
  tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
  });
</script><script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML" async></script><script type="text/javascript" src="/js/codeblock-resizer.js?v=0.0.0"></script><script type="text/javascript" src="/js/smartresize.js?v=0.0.0"></script></div></body></html>