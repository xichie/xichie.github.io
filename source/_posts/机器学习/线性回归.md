---
title: 线性回归
tags:
  - 机器学习
toc: false
comments: true
mathjax: true
categories:
  - 机器学习
abbrlink: '40997091'
date: 2020-05-18 17:11:17
---
## 线性回归  

总结一下机器学习中最常见的模型，首先说一下线性回归模型。  
给定数据集$\{(x^(i), y^(i))\}_{i=1}^{N}$,$x^{(i)}$为d维向量，$y^{(i)}$为实数值。我们现在认为所有的$x^{(i)}$与对应的$y^{(i)}$之间存在一种线性关系。所以我们不妨假设: $$\large y = wx + b$$  

其中$y = \{y^{(1)},y^{(2)}...y^{(N)}\}$,$x = \{x^{(1)},x^{(2)}...x^{(N)}\}$, $w$为d维向量， $b$为标量。
很显然，当给定了数据集，$x,y$就不会改变。所以我们只要找到最好的$w, b$就找到了$x,y$对应的线性关系。  
那么如何判断$w,b$是不是最好的呢？现在随机给一组$\hat{w},\hat{b}$,对每一对$\{x^{(i)},y^{(i)}\}$都可以计算出预测的$\hat{y}$。我们的目的是希望预测的$\hat{y}$和真实数据的$y^{(i)}$能够越接近越好。常用的度量方式就是平方误差：$$L^{(i)} = (\hat y^{(i)} - y^{(i)})^2$$  
对于所有的$\{x^{(i)},y^{(i)}\}$我们计算它们的均方误差（mse）：$$Loss = \frac{1}{N}\sum_{i=0}^{N}\frac{1}{2}(\hat{y}^{(i)} - y^{(i)})^2$$ 
其中乘以1/2是为了求导的方便。
现在我们只要找到使Loss最小的w,b就能够得到我们想要的模型，常用的优化方法就是梯度下降法。  
具体的梯度下降法在另一个博客中做了一个总结。https://blog.csdn.net/qq_26972735/article/details/85049447  
当然我们自然会有一个疑惑，为什么要是误差的平方呢？直接求误差，或者3次方4次方什么的不行吗？首先直接求误差肯定不行（因为不同的数据得到的误差有正有负，会互相抵消。），那么换成绝对值呢？也不是不行，只不过绝对值这个数学操作它不可导，所以我们就不能用梯度下降去优化它，也就是说绝对值优化起来会很困难。而3次方4次方什么的也是因为误差抵消的原因以及求导的计算比较复杂，所以我们一般不用。二次方不存在上述的问题，求导简单，并且有其几何意义，因为它就是在向量空间的中的两个点的欧式距离。由于线性回归很简单所以目前就写这么多了。实验网上有很多带图并且很漂亮的介绍，所以没做。日后有想法再补充吧。。。  
加一个简单的推导过程：
![](https://img-blog.csdnimg.cn/20190314202146502.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzI2OTcyNzM1,size_16,color_FFFFFF,t_70)
![](https://img-blog.csdnimg.cn/20190314202203366.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzI2OTcyNzM1,size_16,color_FFFFFF,t_70)
![](https://img-blog.csdnimg.cn/20190314202213420.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzI2OTcyNzM1,size_16,color_FFFFFF,t_70)
**下一篇总结LogisticRegression。**